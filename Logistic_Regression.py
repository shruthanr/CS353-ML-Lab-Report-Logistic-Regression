# -*- coding: utf-8 -*-
"""Logistic_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://github.com/shruthanr/CS353-ML-Lab-Report-Logistic-Regression/blob/main/Logistic_Regression.ipynb
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# Logistic regression module from sklearn
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

# Loading the dataset
adv_df = pd.read_csv('/advertisement_data.csv')

print(adv_df)

# Set target variable and remove unnecessary columns
targets = adv_df['Clicked on Ad']
data = adv_df.drop(labels=['Unnamed: 0', 'Clicked on Ad'], axis = 1)

# train data : test data == 8:2
train_data, test_data = data[:800], data[800:]
train_targets, test_targets = targets[:800], targets[800:]

print(train_data, train_targets)
print(train_data.describe())

print(test_data, test_targets)
print(test_data.describe())

"""# **Logistic regression implementation using Scikit-Learn library**"""

# Training the classifier
classifier = LogisticRegression(max_iter=1000)
classifier.fit(train_data, train_targets)

# Prediction on test data
pred_targets = classifier.predict(test_data)
print(metrics.classification_report(test_targets, pred_targets))

# Cross validation with number of folds = 5
clf = LogisticRegression(max_iter=1000)
cv_accuracy = cross_val_score(clf, train_data, train_targets, cv=5, scoring='accuracy')
print('Cross validation accuracy: {}'.format(cv_accuracy))
cv_f1score = cross_val_score(clf, train_data, train_targets, cv=5, scoring='f1_macro')
print('Cross validation F1 scores: {}'.format(cv_f1score))

"""# **Logistic regression implementation from scratch without any libraries**"""

# Define the sigmoid function
def sigmoid(h):
    return 1/(1 + np.exp(-h))

# Cross entropy loss (log loss) for two-class classification problem
def compute_loss(true_targets, pred_probs):
    return -np.log(1-pred_probs) * (1-true_targets) - np.log(pred_probs) * true_targets

# Cosntruct the hypothesis to be fed into the sigmoid function (w0x0 + w1x1 + ..... + wNxN)
def construct_hypothesis(parameters, data):
    return np.sum(np.multiply(parameters, data), axis=1)

# Batch gradient descent step to update the parameters
def gradient_descent(parameters, lr, train_data, h, mask):
    for i in range(len(parameters)):
        parameters[i] = parameters[i] - lr * np.mean((mask * train_data[:, i])/(1 + np.exp(-mask * h)))/len(h)

# Scale continuous features to avoid out of range values
scaler = StandardScaler()

if type(train_data) is not np.ndarray:
    train_data = train_data.to_numpy()
    train_targets = train_targets.to_numpy()
    train_data = np.concatenate((scaler.fit_transform(train_data[:,:-1]), train_data[:,-1].reshape(train_data[:,-1].shape[0], 1)), axis=1)

# Initialize parameters randomly
parameters = np.random.rand(train_data.shape[1])
num_train_examples = train_data.shape[0]
# Number of iterations to train
train_iterations = 30000
# Learning rate
lr = 0.5
print('Initial parameters = {}'.format(parameters))

# Plot to visualize the loss as a function of number of iterations
def plot_curve(losses):
    plt.figure(figsize=(8,8))
    plt.plot(losses)
    plt.xlabel('Iterations')
    plt.ylabel('Loss')
    plt.title('Loss vs. #Iterations')
    plt.grid()
    plt.show()

# Main training loop
losses = []
for itr in range(train_iterations):
    # Get hypothesis as a linear combination of parameters and features
    h = construct_hypothesis(parameters, train_data)
    # Get probabilites as predicted by the model
    pred_probs = sigmoid(h)
    # Compute log loss
    loss = compute_loss(train_targets, pred_probs)
    print('Loss: {}'.format(np.mean(loss)))
    losses.append(np.mean(loss))
    # Mask to take care of sign
    mask = np.ones(num_train_examples, dtype=int)
    mask = mask - 2 * train_targets
    # Update parameters
    gradient_descent(parameters, lr, train_data, h, mask)

# Visualization
plot_curve(losses)

print('Parameters after training = {}'.format(parameters))

# Scale the test data accordingly
if type(test_data) is not np.ndarray:
    test_data = test_data.to_numpy()
    test_targets = test_targets.to_numpy()
    test_data = np.concatenate((scaler.fit_transform(test_data[:,:-1]), test_data[:,-1].reshape(test_data[:,-1].shape[0], 1)), axis=1)

# Probabilities predicted by the trained logistic regression model
pred_probs = sigmoid(construct_hypothesis(parameters, test_data))

# Accuracy = (Number of examples correctly classified/Total number of examples) * 100
# Used 0.5 as the classification threshold
print('\033[1m Accuracy of the Logistic regression model implemented from scratch on test data = {} %'.format(np.mean(np.around(pred_probs) == test_targets) * 100))
